{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egibApAsJHq7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mXzrabj_JM-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 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\n",
        "from huggingface_hub import login\n",
        "//createorwriteyourhuggingfacetoken"
      ],
      "metadata": {
        "id": "QwE8r1oEJSF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OpvPHX8vKAOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pandas pyarrow tdqm requests ipywidgets huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3WD_v12KCJW",
        "outputId": "81bd904f-2349-4d64-d87e-27054490ce33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (18.1.0)\n",
            "Collecting tdqm\n",
            "  Downloading tdqm-0.0.1.tar.gz (1.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (7.7.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from tdqm) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.3)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.7)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.8.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (4.3.8)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.24.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.25.1)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.7)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: tdqm\n",
            "  Building wheel for tdqm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tdqm: filename=tdqm-0.0.1-py3-none-any.whl size=1322 sha256=21d775f99ca6fa7ef45e4d0e50a4430ebd1f7bcc926f68212b9383904e018b97\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/c7/30/e5935be2cfa6883be72462333edc414d8928f3c78eaabec38a\n",
            "Successfully built tdqm\n",
            "Installing collected packages: tdqm, jedi\n",
            "Successfully installed jedi-0.19.2 tdqm-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import requests\n",
        "import zipfile\n",
        "import concurrent.futures\n",
        "import pandas as pd\n",
        "from functools import partial\n",
        "from tqdm.auto import tqdm\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import time"
      ],
      "metadata": {
        "id": "qzfyreVjV1XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://data.binance.vision/data/spot/monthly/trades/BTCUSDT/BTCUSDT-trades-2022-01.zip\"\n",
        "output_path = \"BTCUSDT-trades-2022-01.zip\"\n",
        "\n",
        "response = requests.get(url)\n",
        "with open(output_path, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "print(f\"✅ Downloaded: {output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-NzECHcV5Qz",
        "outputId": "27bfc3e5-437a-4b40-ca42-4ed0f5a18ef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Downloaded: BTCUSDT-trades-2022-01.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(output_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"unzipped_files\")"
      ],
      "metadata": {
        "id": "pCUV6DX2V90G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "input_folder = \"unzipped_files\"\n",
        "output_folder = \"partitioned_csvs\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Set chunk size to limit each file (~2M rows)\n",
        "chunk_size = 1_500_000\n",
        "\n",
        "# Process each CSV file during extraction\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        file_path = os.path.join(input_folder, filename)\n",
        "        part_number = 1\n",
        "\n",
        "        # Read and partition into smaller CSVs\n",
        "        for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
        "            output_file = os.path.join(output_folder, f\"{filename}_part_{part_number}.csv\")\n",
        "            chunk.to_csv(output_file, index=False)\n",
        "            print(f\"Saved: {output_file} ({chunk.shape[0]} rows)\")\n",
        "            part_number += 1"
      ],
      "metadata": {
        "id": "CmujjqrOWlda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd3c8509-8f2e-4307-f57f-f91a79b22a2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: partitioned_csvs/BTCUSDT-trades-2022-01.csv_part_1.csv (1500000 rows)\n",
            "Saved: partitioned_csvs/BTCUSDT-trades-2022-01.csv_part_2.csv (1500000 rows)\n",
            "Saved: partitioned_csvs/BTCUSDT-trades-2022-01.csv_part_3.csv (1500000 rows)\n",
            "Saved: partitioned_csvs/BTCUSDT-trades-2022-01.csv_part_4.csv (1500000 rows)\n",
            "Saved: partitioned_csvs/BTCUSDT-trades-2022-01.csv_part_5.csv (1500000 rows)\n",
            "Saved: partitioned_csvs/BTCUSDT-trades-2022-01.csv_part_6.csv (1500000 rows)\n",
            "Saved: partitioned_csvs/BTCUSDT-trades-2022-01.csv_part_7.csv (1500000 rows)\n",
            "Saved: partitioned_csvs/BTCUSDT-trades-2022-01.csv_part_8.csv (1500000 rows)\n",
            "Saved: partitioned_csvs/BTCUSDT-trades-2022-01.csv_part_9.csv (1500000 rows)\n",
            "Saved: partitioned_csvs/BTCUSDT-trades-2022-01.csv_part_10.csv (1500000 rows)\n",
            "Saved: partitioned_csvs/BTCUSDT-trades-2022-01.csv_part_11.csv (1500000 rows)\n",
            "Saved: partitioned_csvs/BTCUSDT-trades-2022-01.csv_part_12.csv (1500000 rows)\n",
            "Saved: partitioned_csvs/BTCUSDT-trades-2022-01.csv_part_13.csv (1500000 rows)\n",
            "Saved: partitioned_csvs/BTCUSDT-trades-2022-01.csv_part_14.csv (1500000 rows)\n",
            "Saved: partitioned_csvs/BTCUSDT-trades-2022-01.csv_part_15.csv (1500000 rows)\n",
            "Saved: partitioned_csvs/BTCUSDT-trades-2022-01.csv_part_16.csv (1500000 rows)\n",
            "Saved: partitioned_csvs/BTCUSDT-trades-2022-01.csv_part_17.csv (1500000 rows)\n",
            "Saved: partitioned_csvs/BTCUSDT-trades-2022-01.csv_part_18.csv (1500000 rows)\n",
            "Saved: partitioned_csvs/BTCUSDT-trades-2022-01.csv_part_19.csv (1500000 rows)\n",
            "Saved: partitioned_csvs/BTCUSDT-trades-2022-01.csv_part_20.csv (1500000 rows)\n",
            "Saved: partitioned_csvs/BTCUSDT-trades-2022-01.csv_part_21.csv (1500000 rows)\n",
            "Saved: partitioned_csvs/BTCUSDT-trades-2022-01.csv_part_22.csv (1500000 rows)\n",
            "Saved: partitioned_csvs/BTCUSDT-trades-2022-01.csv_part_23.csv (1500000 rows)\n",
            "Saved: partitioned_csvs/BTCUSDT-trades-2022-01.csv_part_24.csv (235026 rows)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def process_binance_to_sequences(btc_data, sequence_length=20):\n",
        "    \"\"\"\n",
        "    Convert Binance BTC-USDT data to LSTM-ready sequences\n",
        "    \"\"\"\n",
        "    # Create microstructure features (as discussed in previous steps)\n",
        "    data = btc_data.copy()\n",
        "\n",
        "    # Price-based features\n",
        "    data['price_change'] = data['price'].diff()\n",
        "    data['price_return'] = data['price'].pct_change()\n",
        "    data['log_return'] = np.log(data['price'] / data['price'].shift(1))\n",
        "\n",
        "    # Volume-based features\n",
        "    data['volume_imbalance'] = np.where(data['isBuyerMaker'], -data['qty'], data['qty'])\n",
        "    data['cumulative_volume'] = data['qty'].cumsum()\n",
        "    data['quote_volume_ratio'] = data['quoteQty'] / data['qty']\n",
        "\n",
        "    # Rolling window features\n",
        "    window_size = 50\n",
        "    data['price_volatility'] = data['price_return'].rolling(window=window_size).std()\n",
        "    data['volume_sma'] = data['qty'].rolling(window=window_size).mean()\n",
        "    data['price_sma'] = data['price'].rolling(window=window_size).mean()\n",
        "\n",
        "    # Temporal features\n",
        "    data['hour'] = pd.to_datetime(data['timestamp'], unit='ms').dt.hour\n",
        "    data['minute'] = pd.to_datetime(data['timestamp'], unit='ms').dt.minute\n",
        "    data['day_of_week'] = pd.to_datetime(data['timestamp'], unit='ms').dt.dayofweek\n",
        "\n",
        "    # Market regime indicators\n",
        "    data['price_momentum'] = data['price'] - data['price'].shift(10)\n",
        "    data['volume_momentum'] = data['qty'] - data['qty'].shift(10)\n",
        "\n",
        "    # Create prediction labels (3-class for quantum SVM)\n",
        "    future_returns = data['price'].shift(-10) / data['price'] - 1\n",
        "    data['label'] = np.where(future_returns > 0.001, 2,  # Up\n",
        "                   np.where(future_returns < -0.001, 0, 1))  # Down, Stationary\n",
        "\n",
        "    # Select features for sequences\n",
        "    feature_columns = [\n",
        "        'price', 'qty', 'quoteQty', 'price_change', 'price_return',\n",
        "        'volume_imbalance', 'price_volatility', 'volume_sma', 'price_sma',\n",
        "        'price_momentum', 'volume_momentum', 'hour', 'minute'\n",
        "    ]\n",
        "\n",
        "    # Remove NaN values\n",
        "    data = data.dropna().reset_index(drop=True)\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    features_normalized = scaler.fit_transform(data[feature_columns])\n",
        "    labels = data['label'].values\n",
        "\n",
        "    # Create sequences\n",
        "    sequences = []\n",
        "    sequence_labels = []\n",
        "\n",
        "    for i in range(len(features_normalized) - sequence_length):\n",
        "        sequences.append(features_normalized[i:i+sequence_length])\n",
        "        sequence_labels.append(labels[i+sequence_length])\n",
        "\n",
        "    X_sequences = np.array(sequences)\n",
        "    y_labels = np.array(sequence_labels)\n",
        "\n",
        "    print(f\"Sequences created: {X_sequences.shape}\")\n",
        "    print(f\"Labels created: {y_labels.shape}\")\n",
        "    print(f\"Label distribution: {np.bincount(y_labels)}\")\n",
        "\n",
        "    return X_sequences, y_labels, scaler\n",
        "\n",
        "# Process your Binance data (replace 'btc_data' with your actual dataset variable)\n",
        "# X_sequences, y_labels, data_scaler = process_binance_to_sequences(btc_data)\n"
      ],
      "metadata": {
        "id": "6pYKjKI4YsbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d9f0930-e03a-4350-b75d-8456f2fa897d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_time_series_split(X, y, test_size=0.3):\n",
        "    \"\"\"\n",
        "    Chronological train/test split to avoid look-ahead bias.\n",
        "    \"\"\"\n",
        "    split_index = int(len(X) * (1 - test_size))\n",
        "    X_train, X_test = X[:split_index], X[split_index:]\n",
        "    y_train, y_test = y[:split_index], y[split_index:]\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "3dLxuIrRcduU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_model(seq_length, n_features):\n",
        "    \"\"\"Build LSTM with dynamic input shape and regularization\"\"\"\n",
        "    model = Sequential([\n",
        "        Input(shape=(seq_length, n_features)),\n",
        "        LSTM(64,\n",
        "             dropout=0.4,                                  # Input dropout\n",
        "             recurrent_dropout=0.4,                        # Recurrent state dropout\n",
        "             kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
        "             recurrent_regularizer=tf.keras.regularizers.l2(0.001),\n",
        "             # Weight regularization\n",
        "             return_sequences=True),\n",
        "        Dropout(0.5),\n",
        "        LSTM(32, return_sequences=False),\n",
        "        Dropout(0.5),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(3, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "J03ulc7MeFJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yakO22hN0v5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lstm_model.input_shape)  # Should display a valid shape like (None, seq_len, n_feat)"
      ],
      "metadata": {
        "id": "EPC88dJtLKXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = lstm_model.predict(tf.random.normal((1, seq_len, n_feat)))"
      ],
      "metadata": {
        "id": "Z4t8VlPoNC2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model.summary()"
      ],
      "metadata": {
        "id": "qOvrh68kNEO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def extract_quantum_features(lstm_model, sequences, max_features=10):\n",
        "    \"\"\"Extract features from LSTM for quantum processing\"\"\"\n",
        "\n",
        "    # Ensure the model has been built\n",
        "    if not lstm_model.built:  # ✅ Use the built property instead\n",
        "        print(\"Building model with sample data...\")\n",
        "        _ = lstm_model(sequences[:1])\n",
        "\n",
        "\n",
        "    # Explicitly use the input of the first layer instead of lstm_model.input\n",
        "    feature_extractor = Model(\n",
        "        inputs=lstm_model.layers[0].input,  # Use first layer's input\n",
        "        outputs=lstm_model.layers[-2].output  # Get Dense(16) layer output\n",
        "    )\n",
        "\n",
        "    # Extract features\n",
        "    features = feature_extractor.predict(sequences, verbose=0)\n",
        "\n",
        "    # Normalize for quantum encoding (amplitude encoding requires [-1, 1] range)\n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "    normalized_features = scaler.fit_transform(features)\n",
        "\n",
        "    # Limit features for quantum circuit depth constraints\n",
        "    return normalized_features[:, :max_features], scaler"
      ],
      "metadata": {
        "id": "NFsXT9qPfjpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qiskit==1.4.2\n",
        "!pip install qiskit-machine-learning\n",
        "from qiskit.circuit.library import ZZFeatureMap\n",
        "from qiskit_machine_learning.kernels import FidelityQuantumKernel\n",
        "from qiskit_machine_learning.algorithms import QSVC\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "def create_quantum_svm_component(n_features=10):\n",
        "    \"\"\"Create quantum SVM component compatible with Qiskit 1.43.1\"\"\"\n",
        "\n",
        "    # Design quantum feature map\n",
        "    feature_map = ZZFeatureMap(\n",
        "        feature_dimension=n_features,\n",
        "        reps=2,\n",
        "        entanglement='linear'\n",
        "    )\n",
        "\n",
        "    # Create quantum kernel (fidelity handled automatically)\n",
        "    quantum_kernel = FidelityQuantumKernel(feature_map=feature_map)\n",
        "\n",
        "    # Create quantum and classical SVMs\n",
        "    qsvm = QSVC(quantum_kernel=quantum_kernel)\n",
        "    classical_svm = SVC(kernel='rbf', probability=True)\n",
        "\n",
        "    return qsvm, classical_svm, quantum_kernel\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bf3L9rUhinUX",
        "outputId": "f896fbc8-f0e6-4019-fd1a-92cf15001b57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qiskit==1.4.2\n",
            "  Downloading qiskit-1.4.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting rustworkx>=0.15.0 (from qiskit==1.4.2)\n",
            "  Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.17 in /usr/local/lib/python3.11/dist-packages (from qiskit==1.4.2) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.11/dist-packages (from qiskit==1.4.2) (1.15.3)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.11/dist-packages (from qiskit==1.4.2) (1.13.1)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.11/dist-packages (from qiskit==1.4.2) (0.3.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from qiskit==1.4.2) (2.9.0.post0)\n",
            "Collecting stevedore>=3.0.0 (from qiskit==1.4.2)\n",
            "  Downloading stevedore-5.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from qiskit==1.4.2) (4.14.0)\n",
            "Collecting symengine<0.14,>=0.11 (from qiskit==1.4.2)\n",
            "  Downloading symengine-0.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.0->qiskit==1.4.2) (1.17.0)\n",
            "Collecting pbr>=2.0.0 (from stevedore>=3.0.0->qiskit==1.4.2)\n",
            "  Downloading pbr-6.1.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.3->qiskit==1.4.2) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pbr>=2.0.0->stevedore>=3.0.0->qiskit==1.4.2) (75.2.0)\n",
            "Downloading qiskit-1.4.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stevedore-5.4.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading symengine-0.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pbr-6.1.1-py2.py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: symengine, rustworkx, pbr, stevedore, qiskit\n",
            "Successfully installed pbr-6.1.1 qiskit-1.4.2 rustworkx-0.16.0 stevedore-5.4.1 symengine-0.13.0\n",
            "Collecting qiskit-machine-learning\n",
            "  Downloading qiskit_machine_learning-0.8.3-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: qiskit<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from qiskit-machine-learning) (1.4.2)\n",
            "Requirement already satisfied: numpy>=2.0 in /usr/local/lib/python3.11/dist-packages (from qiskit-machine-learning) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.11/dist-packages (from qiskit-machine-learning) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.2 in /usr/local/lib/python3.11/dist-packages (from qiskit-machine-learning) (1.6.1)\n",
            "Requirement already satisfied: setuptools>=40.1 in /usr/local/lib/python3.11/dist-packages (from qiskit-machine-learning) (75.2.0)\n",
            "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.11/dist-packages (from qiskit-machine-learning) (0.3.7)\n",
            "Requirement already satisfied: rustworkx>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from qiskit<2.0,>=1.0->qiskit-machine-learning) (0.16.0)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.11/dist-packages (from qiskit<2.0,>=1.0->qiskit-machine-learning) (1.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from qiskit<2.0,>=1.0->qiskit-machine-learning) (2.9.0.post0)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from qiskit<2.0,>=1.0->qiskit-machine-learning) (5.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from qiskit<2.0,>=1.0->qiskit-machine-learning) (4.14.0)\n",
            "Requirement already satisfied: symengine<0.14,>=0.11 in /usr/local/lib/python3.11/dist-packages (from qiskit<2.0,>=1.0->qiskit-machine-learning) (0.13.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2->qiskit-machine-learning) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2->qiskit-machine-learning) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.0->qiskit<2.0,>=1.0->qiskit-machine-learning) (1.17.0)\n",
            "Requirement already satisfied: pbr>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from stevedore>=3.0.0->qiskit<2.0,>=1.0->qiskit-machine-learning) (6.1.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.3->qiskit<2.0,>=1.0->qiskit-machine-learning) (1.3.0)\n",
            "Downloading qiskit_machine_learning-0.8.3-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: qiskit-machine-learning\n",
            "Successfully installed qiskit-machine-learning-0.8.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_hybrid_framework(lstm_model, quantum_kernel, X_train, y_train, X_val, y_val):\n",
        "    \"\"\"Complete training pipeline for quantum-classical framework\"\"\"\n",
        "\n",
        "    # 1. Train LSTM model\n",
        "    print(\"Training LSTM component...\")\n",
        "\n",
        "    # Convert labels to categorical\n",
        "    y_train_cat = to_categorical(y_train, num_classes=3)\n",
        "    y_val_cat = to_categorical(y_val, num_classes=3)\n",
        "\n",
        "    # FIXED: Use only proper callbacks\n",
        "    history = lstm_model.fit(\n",
        "        X_train, y_train_cat,\n",
        "        validation_data=(X_val, y_val_cat),\n",
        "        epochs=5,\n",
        "        batch_size=64,\n",
        "        verbose=1,\n",
        "        callbacks=[\n",
        "    tf.keras.callbacks.EarlyStopping\n",
        "     (monitor='val_loss', patience=7, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau\n",
        "    (monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6)\n",
        "]\n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Extracting features for quantum processing...\")\n",
        "    quantum_features_train, scaler = extract_quantum_features(lstm_model, X_train)\n",
        "    quantum_features_val, _ = extract_quantum_features(lstm_model, X_val)\n",
        "\n",
        "    print(\"Training Quantum SVM...\")\n",
        "    qsvm = QSVC(quantum_kernel=quantum_kernel)\n",
        "    qsvm.fit(quantum_features_train, y_train)\n",
        "\n",
        "    classical_svm = SVC(kernel='rbf')\n",
        "    classical_svm.fit(quantum_features_train, y_train)\n",
        "\n",
        "    quantum_accuracy = qsvm.score(quantum_features_val, y_val)\n",
        "    classical_accuracy = classical_svm.score(quantum_features_val, y_val)\n",
        "\n",
        "    print(f\"Quantum SVM Accuracy: {quantum_accuracy:.4f}\")\n",
        "    print(f\"Classical SVM Accuracy: {classical_accuracy:.4f}\")\n",
        "    print(f\"Quantum Advantage: {(quantum_accuracy - classical_accuracy) * 100:.2f}%\")\n",
        "\n",
        "    return {\n",
        "        'lstm_model': lstm_model,\n",
        "        'qsvm': qsvm,\n",
        "        'classical_svm': classical_svm,\n",
        "        'scaler': scaler,\n",
        "        'history': history,\n",
        "        'quantum_accuracy': quantum_accuracy,\n",
        "        'classical_accuracy': classical_accuracy\n",
        "    }"
      ],
      "metadata": {
        "id": "Ydfm7UvDj0JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = \"/content/partitioned_csvs/BTCUSDT-trades-2022-01.csv_part_1.csv\"\n",
        "# 1. Load raw data\n",
        "cols = ['trade_id', 'price', 'qty', 'quoteQty', 'timestamp', 'isBuyerMaker', 'isBestMatch']\n",
        "btc_data = pd.read_csv(csv_path, header=None, names=cols)\n",
        "\n",
        "\n",
        "print(btc_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHlAGBsUkB7r",
        "outputId": "77a13170-b826-4dfb-f877-691268cf9c93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     trade_id     price      qty    quoteQty      timestamp  isBuyerMaker  \\\n",
            "0  1207691977  46216.93  0.00709  327.678034  1640995200000         False   \n",
            "1  1207691978  46216.92  0.00041   18.948937  1640995200000          True   \n",
            "2  1207691979  46216.93  0.00056   25.881481  1640995200000         False   \n",
            "3  1207691980  46216.92  0.00066   30.503167  1640995200000          True   \n",
            "4  1207691981  46216.92  0.00523  241.714492  1640995200002          True   \n",
            "\n",
            "   isBestMatch  \n",
            "0         True  \n",
            "1         True  \n",
            "2         True  \n",
            "3         True  \n",
            "4         True  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "btc_data['datetime'] = pd.to_datetime(btc_data['timestamp'], unit='ms')\n",
        "# 2. Sequence generation\n",
        "X_sequences, y_labels, data_scaler = process_binance_to_sequences(btc_data, sequence_length=20)\n",
        "\n",
        "# 3. Train/test split (70/30)\n",
        "split_idx = int(len(X_sequences) * 0.7)\n",
        "X_train, X_test = X_sequences[:split_idx], X_sequences[split_idx:]\n",
        "y_train, y_test = y_labels[:split_idx], y_labels[split_idx:]\n",
        "\n",
        "# 4. Train/validation split (within training set)\n",
        "val_idx = int(len(X_train) * 0.8)\n",
        "X_train_final, X_val = X_train[:val_idx], X_train[val_idx:]\n",
        "y_train_final, y_val = y_train[:val_idx], y_train[val_idx:]\n"
      ],
      "metadata": {
        "id": "Mv9_HVZUkzY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cd05577-a168-42e9-c1e7-7f4aaa5333bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequences created: (1499931, 20, 13)\n",
            "Labels created: (1499931,)\n",
            "Label distribution: [     63 1499841      27]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.1 Rebuild LSTM dynamically\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dropout, Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "seq_len, n_feat = X_train_final.shape[1], X_train_final.shape[2]\n",
        "lstm_model = build_lstm_model(seq_length=seq_len, n_features=n_feat)\n",
        "_ = lstm_model(tf.random.normal((1, seq_len, n_feat)))  # Define I/O graphs\n",
        "\n",
        "# 6.2 Train hybrid framework\n",
        "quantum_svm, classical_svm, quantum_kernel = create_quantum_svm_component(n_features=10)\n",
        "results = train_hybrid_framework(lstm_model, quantum_kernel, X_train_final, y_train_final, X_val, y_val)\n",
        "print(\"Final Quantum vs Classical Accuracies:\", results['quantum_acc'], results['classical_acc'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izIaif4S9zrV",
        "outputId": "b1979b46-e0a7-47db-c61e-a6141d4af793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LSTM component...\n",
            "Epoch 1/5\n",
            "\u001b[1m13125/13125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1079s\u001b[0m 82ms/step - accuracy: 0.9949 - loss: 0.0228 - val_accuracy: 0.9999 - val_loss: 9.4479e-04 - learning_rate: 0.0010\n",
            "Epoch 2/5\n",
            "\u001b[1m13125/13125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1106s\u001b[0m 82ms/step - accuracy: 0.9999 - loss: 0.0011 - val_accuracy: 0.9999 - val_loss: 8.8531e-04 - learning_rate: 0.0010\n",
            "Epoch 3/5\n",
            "\u001b[1m13125/13125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1064s\u001b[0m 79ms/step - accuracy: 0.9999 - loss: 0.0011 - val_accuracy: 0.9999 - val_loss: 9.5747e-04 - learning_rate: 0.0010\n",
            "Epoch 4/5\n",
            "\u001b[1m13125/13125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1042s\u001b[0m 79ms/step - accuracy: 0.9999 - loss: 9.4934e-04 - val_accuracy: 0.9999 - val_loss: 9.6180e-04 - learning_rate: 0.0010\n",
            "Epoch 5/5\n",
            "\u001b[1m13125/13125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1048s\u001b[0m 80ms/step - accuracy: 0.9999 - loss: 0.0010 - val_accuracy: 0.9999 - val_loss: 8.3119e-04 - learning_rate: 0.0010\n",
            "Extracting features for quantum processing...\n",
            "Training Quantum SVM...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inside train_hybrid_framework after LSTM training\n",
        "qX_train = extract_quantum_features(lstm_model, X_train_final)\n",
        "qX_val   = extract_quantum_features(lstm_model, X_val)\n"
      ],
      "metadata": {
        "id": "WjpmWmNO9z_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_performance_metrics(results, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Calculate comprehensive performance metrics for conference paper\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*20)\n",
        "    print(\"CALCULATING PERFORMANCE METRICS\")\n",
        "    print(\"=\"*20 + \"\\n\")\n",
        "\n",
        "    # Extract models\n",
        "    lstm_model = results['lstm_model']\n",
        "    qsvm = results['qsvm']\n",
        "    classical_svm = results['classical_svm']\n",
        "    scaler = results['scaler']\n",
        "\n",
        "    # Extract quantum features from test set\n",
        "    quantum_features_test, _ = extract_quantum_features(lstm_model, X_test)\n",
        "\n",
        "    # Get predictions\n",
        "    quantum_preds = qsvm.predict(quantum_features_test)\n",
        "    classical_preds = classical_svm.predict(quantum_features_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "\n",
        "    # Accuracy\n",
        "    q_accuracy = accuracy_score(y_test, quantum_preds)\n",
        "    c_accuracy = accuracy_score(y_test, classical_preds)\n",
        "\n",
        "    # Precision, Recall, F1\n",
        "    q_precision, q_recall, q_f1, _ = precision_recall_fscore_support(y_test, quantum_preds, average='weighted')\n",
        "    c_precision, c_recall, c_f1, _ = precision_recall_fscore_support(y_test, classical_preds, average='weighted')\n",
        "\n",
        "    # Confusion matrices\n",
        "    q_cm = confusion_matrix(y_test, quantum_preds)\n",
        "    c_cm = confusion_matrix(y_test, classical_preds)\n",
        "\n",
        "    # Print results\n",
        "    print(\"Quantum SVM Performance:\")\n",
        "    print(f\"  Accuracy: {q_accuracy:.4f}\")\n",
        "    print(f\"  Precision: {q_precision:.4f}\")\n",
        "    print(f\"  Recall: {q_recall:.4f}\")\n",
        "    print(f\"  F1 Score: {q_f1:.4f}\")\n",
        "    print(\"\\nClassical SVM Performance:\")\n",
        "    print(f\"  Accuracy: {c_accuracy:.4f}\")\n",
        "    print(f\"  Precision: {c_precision:.4f}\")\n",
        "    print(f\"  Recall: {c_recall:.4f}\")\n",
        "    print(f\"  F1 Score: {c_f1:.4f}\")\n",
        "\n",
        "    # Calculate quantum advantage\n",
        "    advantage = {\n",
        "        'accuracy': (q_accuracy - c_accuracy) * 100,\n",
        "        'precision': (q_precision - c_precision) * 100,\n",
        "        'recall': (q_recall - c_recall) * 100,\n",
        "        'f1': (q_f1 - c_f1) * 100\n",
        "    }\n",
        "\n",
        "    print(\"\\nQuantum Advantage:\")\n",
        "    print(f\"  Accuracy Improvement: {advantage['accuracy']:.2f}%\")\n",
        "    print(f\"  Precision Improvement: {advantage['precision']:.2f}%\")\n",
        "    print(f\"  Recall Improvement: {advantage['recall']:.2f}%\")\n",
        "    print(f\"  F1 Score Improvement: {advantage['f1']:.2f}%\")\n",
        "\n",
        "    return {\n",
        "        'quantum_metrics': {\n",
        "            'accuracy': q_accuracy,\n",
        "            'precision': q_precision,\n",
        "            'recall': q_recall,\n",
        "            'f1': q_f1,\n",
        "            'confusion_matrix': q_cm\n",
        "        },\n",
        "        'classical_metrics': {\n",
        "            'accuracy': c_accuracy,\n",
        "            'precision': c_precision,\n",
        "            'recall': c_recall,\n",
        "            'f1': c_f1,\n",
        "            'confusion_matrix': c_cm\n",
        "        },\n",
        "        'advantage': advantage\n",
        "    }\n",
        "\n",
        "# Calculate metrics\n",
        "metrics = calculate_performance_metrics(results, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "QXQsV6UJ975Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def implement_portfolio_optimization(results, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Implement Sharpe ratio optimization using quantum predictions\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*20)\n",
        "    print(\"IMPLEMENTING PORTFOLIO OPTIMIZATION\")\n",
        "    print(\"=\"*20 + \"\\n\")\n",
        "\n",
        "    # Extract models\n",
        "    lstm_model = results['lstm_model']\n",
        "    qsvm = results['qsvm']\n",
        "    classical_svm = results['classical_svm']\n",
        "\n",
        "    # Extract quantum features from test set\n",
        "    quantum_features_test, _ = extract_quantum_features(lstm_model, X_test)\n",
        "\n",
        "    # Get prediction probabilities\n",
        "    quantum_probs = qsvm.predict_proba(quantum_features_test)\n",
        "    classical_probs = classical_svm.predict_proba(quantum_features_test)\n",
        "\n",
        "    # Simulate portfolio returns\n",
        "    def simulate_portfolio(predictions, actual, risk_free_rate=0.02):\n",
        "        # Convert predictions to positions (-1, 0, 1)\n",
        "        positions = np.where(predictions == 2, 1, np.where(predictions == 0, -1, 0))\n",
        "\n",
        "        # Simulate daily returns (simplified)\n",
        "        returns = positions * 0.001  # Base return\n",
        "\n",
        "        # Add bonus for correct predictions\n",
        "        correct = (predictions == actual).astype(int)\n",
        "        returns += correct * 0.002  # Bonus for correct predictions\n",
        "\n",
        "        # Calculate Sharpe ratio\n",
        "        excess_returns = returns - (risk_free_rate / 252)  # Daily excess returns\n",
        "        sharpe = np.mean(excess_returns) / np.std(excess_returns) * np.sqrt(252)  # Annualized\n",
        "\n",
        "        # Calculate other metrics\n",
        "        cumulative_return = np.cumprod(1 + returns) - 1\n",
        "        max_drawdown = np.max(np.maximum.accumulate(cumulative_return) - cumulative_return)\n",
        "\n",
        "        return {\n",
        "            'returns': returns,\n",
        "            'cumulative_return': cumulative_return[-1],\n",
        "            'sharpe_ratio': sharpe,\n",
        "            'volatility': np.std(returns) * np.sqrt(252),\n",
        "            'max_drawdown': max_drawdown\n",
        "        }\n",
        "\n",
        "    # Simulate portfolios\n",
        "    quantum_portfolio = simulate_portfolio(qsvm.predict(quantum_features_test), y_test)\n",
        "    classical_portfolio = simulate_portfolio(classical_svm.predict(quantum_features_test), y_test)\n",
        "\n",
        "    # Print results\n",
        "    print(\"Quantum Portfolio Performance:\")\n",
        "    print(f\"  Sharpe Ratio: {quantum_portfolio['sharpe_ratio']:.4f}\")\n",
        "    print(f\"  Cumulative Return: {quantum_portfolio['cumulative_return']*100:.2f}%\")\n",
        "    print(f\"  Volatility: {quantum_portfolio['volatility']*100:.2f}%\")\n",
        "    print(f\"  Maximum Drawdown: {quantum_portfolio['max_drawdown']*100:.2f}%\")\n",
        "\n",
        "    print(\"\\nClassical Portfolio Performance:\")\n",
        "    print(f\"  Sharpe Ratio: {classical_portfolio['sharpe_ratio']:.4f}\")\n",
        "    print(f\"  Cumulative Return: {classical_portfolio['cumulative_return']*100:.2f}%\")\n",
        "    print(f\"  Volatility: {classical_portfolio['volatility']*100:.2f}%\")\n",
        "    print(f\"  Maximum Drawdown: {classical_portfolio['max_drawdown']*100:.2f}%\")\n",
        "\n",
        "    # Calculate improvements\n",
        "    sharpe_improvement = (quantum_portfolio['sharpe_ratio'] - classical_portfolio['sharpe_ratio']) / classical_portfolio['sharpe_ratio'] * 100\n",
        "    return_improvement = (quantum_portfolio['cumulative_return'] - classical_portfolio['cumulative_return']) / classical_portfolio['cumulative_return'] * 100\n",
        "\n",
        "    print(f\"\\nSharpe Ratio Improvement: {sharpe_improvement:.2f}%\")\n",
        "    print(f\"Return Improvement: {return_improvement:.2f}%\")\n",
        "\n",
        "    return {\n",
        "        'quantum_portfolio': quantum_portfolio,\n",
        "        'classical_portfolio': classical_portfolio,\n",
        "        'improvements': {\n",
        "            'sharpe': sharpe_improvement,\n",
        "            'return': return_improvement\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Run portfolio optimization\n",
        "portfolio_results = implement_portfolio_optimization(results, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "LPTAXNDVIjYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Required Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    confusion_matrix\n",
        ")\n",
        "from scipy.stats import kurtosis, skew\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from qiskit.circuit.library import ZZFeatureMap\n",
        "from qiskit_machine_learning.kernels import FidelityQuantumKernel\n",
        "from qiskit_machine_learning.algorithms import QSVC\n",
        "from sklearn.svm import SVC\n",
        "import time\n",
        "\n",
        "# Quantum Resource Metrics\n",
        "def compute_circuit_metrics(feature_map: ZZFeatureMap):\n",
        "    \"\"\"\n",
        "    Compute circuit depth, number of qubits, and approximate gate count.\n",
        "    \"\"\"\n",
        "    depth = feature_map.depth()\n",
        "    num_qubits = feature_map.num_qubits\n",
        "    gate_count = sum(feature_map.count_ops().values())\n",
        "    return depth, num_qubits, gate_count  # [1][2]\n",
        "\n",
        "# Financial Performance Metrics\n",
        "def compute_portfolio_metrics(returns: np.ndarray, risk_free_rate=0.02):\n",
        "    \"\"\"\n",
        "    Compute Sharpe ratio, Calmar ratio, Information ratio.\n",
        "    \"\"\"\n",
        "    # Annualized Sharpe Ratio\n",
        "    mean_ret = np.mean(returns) * 252\n",
        "    vol = np.std(returns) * np.sqrt(252)\n",
        "    sharpe = (mean_ret - risk_free_rate) / vol\n",
        "    # Calmar Ratio\n",
        "    cumulative = np.cumprod(1 + returns) - 1\n",
        "    max_dd = np.max(np.maximum.accumulate(cumulative) - cumulative)\n",
        "    calmar = mean_ret / max_dd\n",
        "    # Information Ratio\n",
        "    benchmark = np.zeros_like(returns)\n",
        "    tracking_err = np.std(returns - benchmark) * np.sqrt(252)\n",
        "    info_ratio = mean_ret / tracking_err\n",
        "    return sharpe, calmar, info_ratio  # [3][4]\n",
        "\n",
        "# Quantum Advantage and Resource Utilization\n",
        "def evaluate_quantum_advantage(\n",
        "    classical_acc: float, quantum_acc: float,\n",
        "    classical_metrics: dict, quantum_metrics: dict\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute percentage improvements and resource efficiency.\n",
        "    \"\"\"\n",
        "    acc_improve = (quantum_acc - classical_acc) * 100\n",
        "    depth, qubits, gates = compute_circuit_metrics(\n",
        "        QuantumFeatureMap(feature_dimension=quantum_metrics['features'], reps=2)\n",
        "    )\n",
        "    resource_eff = qubits / gates\n",
        "    return acc_improve, depth, qubits, gates, resource_eff  # [5][6]\n",
        "\n",
        "# Comprehensive Evaluation Function\n",
        "def comprehensive_evaluation(\n",
        "    y_true: np.ndarray,\n",
        "    preds_quantum: np.ndarray, preds_classical: np.ndarray,\n",
        "    returns_quantum: np.ndarray, returns_classical: np.ndarray,\n",
        "    feature_map: ZZFeatureMap\n",
        "):\n",
        "    \"\"\"\n",
        "    Run full suite of metrics for quantum-classical HFT framework.\n",
        "    \"\"\"\n",
        "    # Classification Metrics\n",
        "    q_acc = accuracy_score(y_true, preds_quantum)\n",
        "    c_acc = accuracy_score(y_true, preds_classical)\n",
        "    q_prec, q_rec, q_f1, _ = precision_recall_fscore_support(\n",
        "        y_true, preds_quantum, average='weighted'\n",
        "    )\n",
        "    c_prec, c_rec, c_f1, _ = precision_recall_fscore_support(\n",
        "        y_true, preds_classical, average='weighted'\n",
        "    )\n",
        "    q_cm = confusion_matrix(y_true, preds_quantum)\n",
        "    c_cm = confusion_matrix(y_true, preds_classical)\n",
        "\n",
        "    # Portfolio Metrics\n",
        "    q_sharpe, q_calmar, q_info = compute_portfolio_metrics(returns_quantum)\n",
        "    c_sharpe, c_calmar, c_info = compute_portfolio_metrics(returns_classical)\n",
        "\n",
        "    # Quantum Resource and Advantage\n",
        "    depth, qubits, gates = feature_map.depth(), feature_map.num_qubits, sum(feature_map.count_ops().values())\n",
        "    acc_improve, _, _, _, resource_eff = evaluate_quantum_advantage(\n",
        "        c_acc, q_acc,\n",
        "        {'features': feature_map.num_qubits},\n",
        "        {'features': feature_map.num_qubits}\n",
        "    )\n",
        "\n",
        "    # Compile Results\n",
        "    results = {\n",
        "        'classification': {\n",
        "            'quantum': {'accuracy': q_acc, 'precision': q_prec, 'recall': q_rec, 'f1': q_f1, 'confusion_matrix': q_cm},\n",
        "            'classical': {'accuracy': c_acc, 'precision': c_prec, 'recall': c_rec, 'f1': c_f1, 'confusion_matrix': c_cm},\n",
        "        },\n",
        "        'portfolio': {\n",
        "            'quantum': {'sharpe': q_sharpe, 'calmar': q_calmar, 'information_ratio': q_info},\n",
        "            'classical': {'sharpe': c_sharpe, 'calmar': c_calmar, 'information_ratio': c_info},\n",
        "        },\n",
        "        'quantum_resources': {'depth': depth, 'qubits': qubits, 'gates': gates, 'resource_efficiency': resource_eff},\n",
        "        'quantum_advantage': {'accuracy_improvement_pct': acc_improve},\n",
        "        'timing': {'evaluation_time_sec': time.time()},\n",
        "    }\n",
        "    return results  # [7][8]\n",
        "\n",
        "# Example Usage\n",
        "# feature_map = ZZFeatureMap(feature_dimension=10, reps=2)\n",
        "# results = comprehensive_evaluation(\n",
        "#     y_test, quantum_preds, classical_preds,\n",
        "#     returns_quantum, returns_classical, feature_map\n",
        "# )\n"
      ],
      "metadata": {
        "id": "0Vq79BAwIlKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cDpwePkWIn8q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}